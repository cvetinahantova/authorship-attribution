Fine, you can tell me that it's important because it proves that Insertion Sort is crappier than Merge Sort. I got that. But what's the fucking point in trying to prove it over and over again in a myriad of new and painfully tortuous ways? It's not practical, I'm not going to spend 2 hours trying to come up with the efficiency of the code that I spent 30 minutes on. Neither am I going to go and code my own sorting algorithms - That's what libraries are for.  But what really,  really  pisses me off is the fucking textbook. They use the worst fucking notation I've ever seen in a book, and I've seen a lot of pretty shitty stuff. For example,    they use  lg n  as a shortcut of  log2 n . Now I don't know how that goes hereabouts, but I was raised to know that  lg n  is a shortcut for  log10 n , and  log10 n  only.   They show some pseudocode, and only show the pseudocode conventions two pages afer that.   They use some weird-ass signs to signify comments and assignments that no-one used before, because they're not standard ASCII characters. And then they claim that they're using notation common to languages like Pascal, C, and Java. Fuckign liars.  They change the meaning of notations. When could you ever see a notation that uses  f[x]  to say that  f  is a field of the  x  object? Only in this shitty book. Everyone sensible would wather use something like  x->f , so as to prevent confusion with the array notation.   The book tries to use buzz-words, but ends up fucking up horribly. They even try to describe a  von Neumann  architecture as a, get a load of this,   generic one-processor, random access machine (RAM) model  . I was confused. Was this a fuckign joke? Or were they really that stupid? Sure, random access machines are not made up, but they just shouldn't throw the word out and then make up their own shitty definition.  And this is a book from the MIT press. What has the world come to? Eh, fuck it all.